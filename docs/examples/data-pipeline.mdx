---
title: Data Pipeline
description: ETL pipeline with error handling and recovery
---

# Data Pipeline

This example demonstrates a data processing pipeline (Extract, Transform, Load) with robust error handling and recovery capabilities.

## Complete Example

```tsx
import { create } from 'zustand'
import { Claude, Phase, Step, Persona, Constraints, OutputFormat } from 'smithers'

// Type definitions
interface DataRecord {
  id: string
  [key: string]: any
}

interface ValidationResult {
  valid: boolean
  errors: Array<{ field: string; message: string }>
}

interface PipelineError {
  stage: string
  message: string
  recoverable: boolean
  context: any
}

// State management
const usePipelineStore = create<{
  stage: 'extract' | 'transform' | 'validate' | 'load' | 'complete' | 'error'
  data: DataRecord[]
  transformedData: DataRecord[]
  validationResult: ValidationResult | null
  error: PipelineError | null
  retryCount: number

  setData: (data: DataRecord[]) => void
  setTransformedData: (data: DataRecord[]) => void
  setValidationResult: (result: ValidationResult) => void
  setError: (error: PipelineError) => void
  complete: () => void
  retry: () => void
}>((set, get) => ({
  stage: 'extract',
  data: [],
  transformedData: [],
  validationResult: null,
  error: null,
  retryCount: 0,

  setData: (data) => set({ data, stage: 'transform' }),

  setTransformedData: (transformedData) =>
    set({ transformedData, stage: 'validate' }),

  setValidationResult: (result) =>
    set({
      validationResult: result,
      stage: result.valid ? 'load' : 'error',
      error: result.valid
        ? null
        : {
            stage: 'validate',
            message: `Validation failed: ${result.errors.length} errors`,
            recoverable: true,
            context: result.errors,
          },
    }),

  setError: (error) => set({ error, stage: 'error' }),

  complete: () => set({ stage: 'complete' }),

  retry: () => {
    const { retryCount, error } = get()
    if (error?.recoverable && retryCount < 3) {
      set({
        retryCount: retryCount + 1,
        error: null,
        stage: error.stage as any,
      })
    }
  },
}))

// Data engineer persona
function DataEngineer() {
  return (
    <Persona role="Data Engineer">
      You are an experienced data engineer specializing in ETL pipelines.
      You handle edge cases gracefully, validate data thoroughly,
      and ensure data quality throughout the pipeline.
    </Persona>
  )
}

// Main pipeline agent
export function DataPipeline({
  source,
  destination,
  transformRules,
}: {
  source: { type: string; config: any }
  destination: { type: string; config: any }
  transformRules?: string[]
}) {
  const {
    stage,
    data,
    transformedData,
    error,
    retryCount,
    setData,
    setTransformedData,
    setValidationResult,
    setError,
    complete,
    retry,
  } = usePipelineStore()

  // Error handling stage
  if (stage === 'error' && error) {
    if (error.recoverable && retryCount < 3) {
      return (
        <Claude onFinished={retry}>
          <DataEngineer />

          <Phase name="error-recovery">
            An error occurred in the {error.stage} stage:
            {error.message}

            Context: {JSON.stringify(error.context)}

            Attempt {retryCount + 1} of 3.

            <Step>Analyze the error</Step>
            <Step>Determine if data can be fixed</Step>
            <Step>Apply corrections if possible</Step>
          </Phase>

          <OutputFormat>
            Return "retry" if the issue can be resolved, or describe why it cannot.
          </OutputFormat>
        </Claude>
      )
    }

    // Non-recoverable error
    return (
      <Claude>
        <Phase name="error-report">
          Pipeline failed permanently after {retryCount} retries.

          Error: {error.message}
          Stage: {error.stage}
          Context: {JSON.stringify(error.context)}

          Please investigate and fix the source data or configuration.
        </Phase>
      </Claude>
    )
  }

  // Stage 1: Extract
  if (stage === 'extract') {
    return (
      <Claude
        tools={[database, api, filesystem]}
        onFinished={(result: { data: DataRecord[] }) => setData(result.data)}
        onError={(err) =>
          setError({
            stage: 'extract',
            message: err.message,
            recoverable: true,
            context: { source },
          })
        }
      >
        <DataEngineer />

        <Constraints>
          - Handle pagination if the source has many records
          - Respect rate limits for API sources
          - Log progress for large extractions
          - Fail gracefully if source is unavailable
        </Constraints>

        <Phase name="extract">
          <Step>Connect to source: {source.type}</Step>
          <Step>Extract all records with config: {JSON.stringify(source.config)}</Step>
          <Step>Handle any connection or data issues</Step>
        </Phase>

        <OutputFormat
          schema={{
            data: [{ id: 'string', '...fields': 'any' }],
            recordCount: 'number',
            extractedAt: 'ISO date string',
          }}
        >
          Return JSON with extracted data.
        </OutputFormat>
      </Claude>
    )
  }

  // Stage 2: Transform
  if (stage === 'transform') {
    return (
      <Claude
        onFinished={(result: { data: DataRecord[] }) =>
          setTransformedData(result.data)
        }
        onError={(err) =>
          setError({
            stage: 'transform',
            message: err.message,
            recoverable: true,
            context: { recordCount: data.length },
          })
        }
      >
        <DataEngineer />

        <Constraints>
          - Preserve data integrity during transformation
          - Handle null/undefined values gracefully
          - Log any records that couldn't be transformed
          - Maintain referential integrity
        </Constraints>

        <Phase name="transform">
          <Step>Apply transformations to {data.length} records</Step>

          {transformRules?.map((rule, i) => (
            <Step key={i}>Apply rule: {rule}</Step>
          ))}

          <Step>Standardize date formats to ISO 8601</Step>
          <Step>Normalize text fields (trim, lowercase where appropriate)</Step>
          <Step>Handle missing values according to field type</Step>
        </Phase>

        Input data sample (first 3 records):
        {JSON.stringify(data.slice(0, 3), null, 2)}

        <OutputFormat
          schema={{
            data: [{ id: 'string', '...fields': 'any' }],
            transformedCount: 'number',
            skippedCount: 'number',
            skippedReasons: ['string'],
          }}
        >
          Return JSON with transformed data.
        </OutputFormat>
      </Claude>
    )
  }

  // Stage 3: Validate
  if (stage === 'validate') {
    return (
      <Claude onFinished={setValidationResult}>
        <DataEngineer />

        <Constraints>
          - Check for required fields
          - Validate data types
          - Check referential integrity
          - Identify duplicates
          - Flag anomalies
        </Constraints>

        <Phase name="validate">
          <Step>Check all required fields are present</Step>
          <Step>Validate data types match schema</Step>
          <Step>Check for duplicate records</Step>
          <Step>Validate referential integrity</Step>
          <Step>Flag statistical anomalies</Step>
        </Phase>

        Validate these {transformedData.length} records:
        {JSON.stringify(transformedData.slice(0, 5), null, 2)}
        ... and {transformedData.length - 5} more

        <OutputFormat
          schema={{
            valid: 'boolean',
            errors: [{ field: 'string', message: 'string', recordId: 'string' }],
            warnings: [{ field: 'string', message: 'string' }],
            stats: {
              totalRecords: 'number',
              validRecords: 'number',
              duplicates: 'number',
            },
          }}
        >
          Return JSON with validation results.
        </OutputFormat>
      </Claude>
    )
  }

  // Stage 4: Load
  if (stage === 'load') {
    return (
      <Claude
        tools={[database]}
        onFinished={complete}
        onError={(err) =>
          setError({
            stage: 'load',
            message: err.message,
            recoverable: false, // DB errors often not recoverable without intervention
            context: { destination, recordCount: transformedData.length },
          })
        }
      >
        <DataEngineer />

        <Constraints>
          - Use transactions for atomicity
          - Batch inserts for performance
          - Handle conflicts according to destination config
          - Log all operations for audit trail
        </Constraints>

        <Phase name="load">
          <Step>Connect to destination: {destination.type}</Step>
          <Step>Begin transaction</Step>
          <Step>Load {transformedData.length} records in batches</Step>
          <Step>Commit transaction</Step>
          <Step>Verify record counts match</Step>
        </Phase>

        Destination config: {JSON.stringify(destination.config)}

        <OutputFormat
          schema={{
            success: 'boolean',
            loadedCount: 'number',
            duration: 'number (ms)',
            destination: 'string',
          }}
        >
          Return JSON with load results.
        </OutputFormat>
      </Claude>
    )
  }

  // Complete
  return null
}
```

## Running the Pipeline

```bash
# Preview
smithers plan data-pipeline.tsx --props '{
  "source": {"type": "postgres", "config": {"table": "users"}},
  "destination": {"type": "bigquery", "config": {"dataset": "analytics"}},
  "transformRules": ["normalize_emails", "parse_addresses"]
}'

# Execute with timeout for long-running pipelines
smithers run data-pipeline.tsx \
  --props '{"source": {...}, "destination": {...}}' \
  --timeout 600000 \
  --verbose
```

## Execution Flow

```
Frame 1: Extract
  └─ Pull data from source (postgres, API, files, etc.)

Frame 2: Transform
  └─ Apply transformation rules

Frame 3: Validate
  └─ Check data quality
  └─ If invalid → go to error stage

Frame 4: Load (or Error Recovery)
  └─ Insert into destination
  └─ Retry up to 3 times on failure

Frame 5: Complete (or Error Report)
```

## Key Patterns

<CardGroup cols={2}>
  <Card title="Stage Machine" icon="diagram-project">
    Clear stages with explicit transitions
  </Card>
  <Card title="Error Recovery" icon="rotate">
    Automatic retries for recoverable errors
  </Card>
  <Card title="Validation Gate" icon="shield-check">
    Data quality checks before loading
  </Card>
  <Card title="Audit Trail" icon="list">
    Detailed logging at each stage
  </Card>
</CardGroup>

## Adding Checkpoints

For very long pipelines, save progress:

```tsx
import { persist } from 'zustand/middleware'

const usePipelineStore = create(
  persist(
    (set, get) => ({
      // ... state and actions
    }),
    {
      name: 'pipeline-checkpoint',
      // Only persist certain fields
      partialize: (state) => ({
        stage: state.stage,
        data: state.data,
        transformedData: state.transformedData,
      }),
    }
  )
)
```

## Related Examples

<CardGroup cols={2}>
  <Card title="Code Review" icon="magnifying-glass" href="/examples/code-review">
    Parallel processing pattern
  </Card>
  <Card title="Multi-Agent Team" icon="users" href="/examples/multi-agent">
    Complex multi-phase workflow
  </Card>
</CardGroup>
